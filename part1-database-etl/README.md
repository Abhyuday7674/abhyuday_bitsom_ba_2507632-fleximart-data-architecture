# ğŸ›’ FlexiMart Data Architecture Project  
## Part 1 â€“ Database ETL Pipeline

---

## ğŸ“Œ Project Overview
This project implements a **complete end-to-end ETL (Extract, Transform, Load) pipeline** for the FlexiMart retail dataset.  
The goal is to ingest raw CSV files, clean and standardize the data, and load it into a **MySQL relational database** following data engineering best practices.

This repository contains the deliverables for:
- **Task 1.1 â€“ ETL Pipeline**
- **Task 1.2 â€“ Database Schema Documentation**
- **Task 1.3 â€“ Business SQL Queries**

---

## ğŸ¯ Objectives
- Extract raw data from multiple CSV files
- Handle real-world data quality issues
- Transform and standardize inconsistent data
- Load clean data into a normalized database
- Generate a data quality report
- Answer business questions using SQL

---

## ğŸ—‚ï¸ Datasets Used

| Dataset | Description |
|------|------------|
| `customers_raw.csv` | Customer demographic and registration details |
| `products_raw.csv` | Product catalog with pricing and inventory |
| `sales_raw.csv` | Transaction-level sales data |

All raw datasets are stored in the root `data/` directory.

---

## ğŸ”„ ETL Pipeline Design

### 1ï¸âƒ£ Extract
- CSV files are read using **Pandas**
- Handles malformed CSV files (single-column loading issue)
- Counts total records read for reporting

---

### 2ï¸âƒ£ Transform

#### ğŸ‘¤ Customers
- Removed duplicate records
- Removed records with missing email addresses
- Standardized phone numbers to `+91XXXXXXXXXX`
- Normalized city names
- Converted registration dates to `YYYY-MM-DD`

#### ğŸ“¦ Products
- Standardized product categories (Electronics, Fashion, Groceries)
- Removed records with missing prices
- Filled missing stock quantities with `0`
- Trimmed extra spaces in product names

#### ğŸ’° Sales
- Removed duplicate transaction IDs
- Dropped records with missing customer or product IDs
- Standardized inconsistent date formats
- Converted numeric fields safely
- Normalized transaction status values

---

### 3ï¸âƒ£ Load
- Data loaded into a **MySQL database (`fleximart`)**
- Tables created using normalized schema design
- Auto-increment surrogate keys used
- ETL pipeline is **safe to re-run** using `TRUNCATE`
- Parameterized SQL queries used for secure insertion

---

## ğŸ§± Database Tables Created
- `customers`
- `products`
- `sales`

Detailed schema documentation is available in:  
ğŸ“„ `schema_documentation.md`

---

## ğŸ“Š Data Quality Report
After ETL execution, a detailed data quality report is generated automatically.

ğŸ“„ File: `data_quality_report.txt`

The report includes:
- Records read per dataset
- Duplicates removed
- Missing values handled
- Records successfully loaded

---

## ğŸ“ Repository Structure

```
abhyuday-fleximart-data-architecture/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ customers_raw.csv
â”‚   â”œâ”€â”€ products_raw.csv
â”‚   â””â”€â”€ sales_raw.csv
â”‚
â””â”€â”€ part1-database-etl/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ etl_pipeline.py
    â”œâ”€â”€ schema_documentation.md
    â”œâ”€â”€ business_queries.sql
    â”œâ”€â”€ data_quality_report.txt
    â””â”€â”€ requirements.txt
```

---

## â–¶ï¸ How to Run the ETL Pipeline

### 1ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Configure MySQL
Update database credentials in `etl_pipeline.py` if required:
```python
host="localhost"
user="root"
password="PASSWORD"
database="fleximart"
```

### 3ï¸âƒ£ Run the ETL pipeline
```bash
python etl_pipeline.py
```

---

## âœ… Sample Execution Output
```
CUSTOMER ETL STARTED
Customers loaded: 20

PRODUCT ETL STARTED
Products loaded: 17

SALES ETL STARTED
Sales loaded: 35

ETL PIPELINE COMPLETED SUCCESSFULLY
```

---

## ğŸ“ˆ Business Query Implementation
Business SQL queries are implemented in:

ğŸ“„ `business_queries.sql`

The queries answer:
1. Customer purchase history
2. Product category sales analysis
3. Monthly sales trends with cumulative revenue

---

## ğŸ§  Technologies Used
- Python 3.x
- Pandas
- MySQL
- mysql-connector-python
- python-dateutil
- SQL

---

## ğŸ“ Evaluation Criteria Mapping

| Criteria | Status |
|-------|-------|
| Extract Logic | âœ… Implemented |
| Transform Logic | âœ… Handles all data issues |
| Load Logic | âœ… MySQL insertion successful |
| Code Quality | âœ… Clean and readable |
| Documentation | âœ… Complete and examiner-friendly |

---

## ğŸ“Œ Notes
- ETL pipeline is fully reproducible
- No manual preprocessing required
- Designed for academic evaluation and real-world ETL standards

---


